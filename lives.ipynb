{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.metrics import average_precision_score, accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/ayp/Projects/attribute-estimation/logssmaple.pkl', 'rb') as f:\n",
    "    lossmaple = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = lossmaple['pred']\n",
    "gt = lossmaple['gt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 1.],\n",
       "       [0., 0., 0., ..., 0., 1., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def example_based(preds, labels, eps=1e-10):\n",
    "  \"\"\"\n",
    "  Calculate instance-based evaluation metric mA\n",
    "\n",
    "  Input\n",
    "  - preds: (N, num_classes)\n",
    "  - labels: (N, num_classes)\n",
    "\n",
    "  Output\n",
    "  - Acc, Pre, Rec, F1 (N,)\n",
    "  \"\"\"\n",
    "  num_instance = labels.shape[0]\n",
    "  acc = np.zeros(num_instance)\n",
    "  prec = np.zeros(num_instance)\n",
    "  recall = np.zeros(num_instance)\n",
    "  f1 = np.zeros(num_instance)\n",
    "\n",
    "  for i in range(num_instance):\n",
    "    preds_i = preds[i] > 0.5\n",
    "    labels_i = labels[i]\n",
    "\n",
    "    # true positive\n",
    "    tp = np.sum((preds_i == 1) & (labels_i == 1))\n",
    "    # true negative\n",
    "    tn = np.sum((preds_i == 0) & (labels_i == 0))\n",
    "    # false positive\n",
    "    fp = np.sum((preds_i == 1) & (labels_i == 0))\n",
    "    # false negative\n",
    "    fn = np.sum((preds_i == 0) & (labels_i == 1))\n",
    "\n",
    "    acc[i] = (tp + tn) / (tp + tn + fp + fn + eps)\n",
    "    prec[i] = tp / (tp + fp + eps)\n",
    "    recall[i] = tp / (tp + fn + eps)\n",
    "    f1[i] = 2 * prec[i] * recall[i] / (prec[i] + recall[i] + eps)\n",
    "\n",
    "  return acc, prec, recall, f1\n",
    "\n",
    "\n",
    "def mean_accuracy(preds, labels, eps=1e-10):\n",
    "  \"\"\"\n",
    "  Calculate label-based evaluation metric mA\n",
    "\n",
    "  Input\n",
    "  - preds: (N, num_classes) np.array\n",
    "  - labels: (N, num_classes) np.array\n",
    "\n",
    "  Output\n",
    "  - mA: (num_classes,)\n",
    "  \"\"\"\n",
    "  num_classes = labels.shape[1]\n",
    "  acc = np.zeros(num_classes)\n",
    "\n",
    "  for i in range(num_classes):\n",
    "    preds_i = preds[:, i] > 0.5\n",
    "    labels_i = labels[:, i]\n",
    "\n",
    "    # true positive\n",
    "    tp = np.sum((preds_i == 1) & (labels_i == 1))\n",
    "    # true negative\n",
    "    tn = np.sum((preds_i == 0) & (labels_i == 0))\n",
    "    # positive\n",
    "    p = np.sum(labels_i == 1)\n",
    "    # negative\n",
    "    n = np.sum(labels_i == 0)\n",
    "\n",
    "    acc[i] = tp / (p + eps) + tn / (n + eps)\n",
    "    acc[i] = acc[i] / 2\n",
    "\n",
    "  return acc\n",
    "\n",
    "def mean_accuracy_skt(preds, labels, eps=1e-10):\n",
    "  \"\"\"\n",
    "  Calculate label-based evaluation metric mA\n",
    "\n",
    "  Input\n",
    "  - preds: (N, num_classes) np.array\n",
    "  - labels: (N, num_classes) np.array\n",
    "\n",
    "  Output\n",
    "  - mA: (num_classes,)\n",
    "  \"\"\"\n",
    "  num_classes = labels.shape[1]\n",
    "  acc = np.zeros(num_classes)\n",
    "\n",
    "  for i in range(num_classes):\n",
    "    preds_i = preds[:, i] \n",
    "    labels_i = labels[:, i]\n",
    "\n",
    "    acc[i] = average_precision_score(labels_i, preds_i)\n",
    "\n",
    "  return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc, prec, recall, f1 = example_based(pred, gt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5900138748354136"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1.])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(gt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gt.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_prec_skt = mean_accuracy_skt(pred, gt)\n",
    "avg_prec = mean_accuracy(pred, gt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skt 0.6807688143322135\n",
      "ours 0.7017464027024299\n"
     ]
    }
   ],
   "source": [
    "print('skt', avg_prec_skt.mean())\n",
    "print('ours', avg_prec.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
